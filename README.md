# Project Overview:
This project aims to gain a comprehensive understanding of Large Language Models (LLMs) by implementing a transformer-based model from scratch. It serves as an educational exploration of the architecture, components, and intricacies of LLMs, providing hands-on experience in designing, coding, and optimizing these models.

## Key Objectives:
### 1.Understand Transformer Architecture:
Explore the encoder-decoder structure, including its critical components.

### 2.Implement the model in a modular fashion to:
Enhance code readability and maintainability.
Allow for easy debugging and further development.

### 3.Optimization with Parallel Computing:
Leverage parallel computing with SparkX to enhance model efficiency during the early stages of training and testing.

The project serves as a learning tool, providing insights into the structure and mechanics of LLMs.

